gpus: 1
epochs: 500
augmentation: False
batch_size: 2
segment_seconds: 10
frames_per_second: 100
val_frequency: 5
num_workers: 8
straight_through: False 
lr: 1e-4
source: True
inst_sampler:
    mode: 'imbalance'
    temp: 0.5
    samples: 3
    neg_samples: 0
    audio_noise: 0.0
    
transcription_weights: '/opt/tiger/kinwai/jointist/weights/transcription1000.ckpt' # for loading pretrained transcription
checkpoint_path: './weights/tseparation.ckpt'

datamodule:
    waveform_hdf5s_dir:
    notes_pkls_dir:    
    dataset_cfg:
        train:
            segment_seconds: ${segment_seconds}
            frames_per_second: ${frames_per_second}
            pre_load_audio: False
            transcription: True
            random_crop: True
            source: ${source}
        val:
            segment_seconds: ${segment_seconds}
            frames_per_second: ${frames_per_second}
            pre_load_audio: False
            transcription: True
            random_crop: False
            source: ${source}            
        test:
            segment_seconds: null
            frames_per_second: ${frames_per_second}
            pre_load_audio: False
            transcription: True
            random_crop: False
            source: ${source}            
    dataloader_cfg:
        train:
            batch_size: ${batch_size}
            num_workers: ${num_workers}
            shuffle: True
            pin_memory: True
        val:
            batch_size: ${batch_size}
            num_workers: ${num_workers}
            shuffle: False
            pin_memory: True
        test:
            batch_size: 1
            num_workers: ${num_workers}
            shuffle: False
            pin_memory: True
            
MIDI_MAPPING: # This whole part will be overwritten in the main code 
    type: 'MIDI_class'
    plugin_labels_num: 0
    NAME_TO_IX: 0
    IX_TO_NAME: 0

checkpoint:
  monitor: 'Total_Loss/Valid'
  filename: "e={epoch:02d}-trainloss={Total_Loss/Train:.3f}-validloss{Total_Loss/Valid:.3f}"
  save_top_k: 1
  mode: 'min'
  save_last: True
  every_n_epochs: ${trainer.check_val_every_n_epoch}

trainer:
  checkpoint_callback: True
  gpus: ${gpus}
  accelerator: 'ddp'
  sync_batchnorm: True
  max_epochs: ${epochs}
  replace_sampler_ddp: False
  profiler: 'simple'
  check_val_every_n_epoch: ${val_frequency}
  num_sanity_val_steps: 2
  resume_from_checkpoint: False 

defaults:
  - transcription: Original  
  - separation: TCUNet
  - scheduler: LambdaLR    
